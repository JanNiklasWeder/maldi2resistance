{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d89aed94233e3ff",
   "metadata": {},
   "source": [
    "# Fully connected feedforward network implementing a loss mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaaf570-0600-4d60-9fde-fcebcff3ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics.classification\n",
    "from torch import cuda\n",
    "assert cuda.is_available()\n",
    "assert cuda.device_count() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e50fb9-601f-425a-bf79-6441170b27dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "print(cuda.get_device_name(cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b645f-6b5e-428e-855c-64b7e33ff874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a6650-ef2b-44f1-9ea8-78912b15e9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff5acd5e3f0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\")\n",
    "SEED = 76436278\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b5a248-98cc-42cc-99b2-49bdd5ca1f35",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00707ac6b2975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27446/27446 [00:06<00:00, 3951.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <table>\n",
       "            <thead>\n",
       "                <tr>\n",
       "                    <th>Antibiotic:</th>\n",
       "                    <th> Amikacin </th><th> Amoxicillin-Clavulanic acid </th><th> Ampicillin </th><th> Ampicillin-Amoxicillin </th><th> Benzylpenicillin </th><th> Cefazolin </th><th> Cefepime </th><th> Cefpodoxime </th><th> Ceftazidime </th><th> Ceftriaxone </th><th> Cefuroxime </th><th> Ciprofloxacin </th><th> Clarithromycin </th><th> Clindamycin </th><th> Colistin </th><th> Cotrimoxazole </th><th> Ertapenem </th><th> Erythromycin </th><th> Fosfomycin </th><th> Fosfomycin-Trometamol </th><th> Fusidic acid </th><th> Gentamicin </th><th> Imipenem </th><th> Levofloxacin </th><th> Meropenem </th><th> Mupirocin </th><th> Nitrofurantoin </th><th> Norfloxacin </th><th> Oxacillin </th><th> Penicillin </th><th> Piperacillin-Tazobactam </th><th> Polymyxin B </th><th> Tetracycline </th><th> Tobramycin </th>\n",
       "                </tr>\n",
       "            </thead>\n",
       "            <tbody>\n",
       "                <tr>\n",
       "                    <td>Number resistant:</td>\n",
       "                    <td> 267 </td><td> 6120 </td><td> 8578 </td><td> 5928 </td><td> 618 </td><td> 1032 </td><td> 2456 </td><td> 677 </td><td> 1649 </td><td> 3122 </td><td> 2412 </td><td> 3629 </td><td> 260 </td><td> 1625 </td><td> 837 </td><td> 3806 </td><td> 204 </td><td> 2047 </td><td> 2264 </td><td> 371 </td><td> 1072 </td><td> 1706 </td><td> 2592 </td><td> 1315 </td><td> 1291 </td><td> 549 </td><td> 954 </td><td> 563 </td><td> 2021 </td><td> 5042 </td><td> 2732 </td><td> 486 </td><td> 1140 </td><td> 385 </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td>Number susceptible:</td>\n",
       "                    <td> 9321 </td><td> 13875 </td><td> 4194 </td><td> 1440 </td><td> 457 </td><td> 1708 </td><td> 15856 </td><td> 1272 </td><td> 13796 </td><td> 14520 </td><td> 3977 </td><td> 20191 </td><td> 1061 </td><td> 4066 </td><td> 4516 </td><td> 8729 </td><td> 11251 </td><td> 4273 </td><td> 10161 </td><td> 1349 </td><td> 3300 </td><td> 16625 </td><td> 15481 </td><td> 5889 </td><td> 9629 </td><td> 2053 </td><td> 2422 </td><td> 3269 </td><td> 3255 </td><td> 1623 </td><td> 15571 </td><td> 2305 </td><td> 5538 </td><td> 4974 </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td>Number data points:</td>\n",
       "                    <td> 9588 </td><td> 19995 </td><td> 12772 </td><td> 7368 </td><td> 1075 </td><td> 2740 </td><td> 18312 </td><td> 1949 </td><td> 15445 </td><td> 17642 </td><td> 6389 </td><td> 23820 </td><td> 1321 </td><td> 5691 </td><td> 5353 </td><td> 12535 </td><td> 11455 </td><td> 6320 </td><td> 12425 </td><td> 1720 </td><td> 4372 </td><td> 18331 </td><td> 18073 </td><td> 7204 </td><td> 10920 </td><td> 2602 </td><td> 3376 </td><td> 3832 </td><td> 5276 </td><td> 6665 </td><td> 18303 </td><td> 2791 </td><td> 6678 </td><td> 5359 </td>\n",
       "                </tr>\n",
       "            </tbody>\n",
       "        </table>\n",
       "                "
      ],
      "text/plain": [
       "<src.maldi2resistance.data.driams.Driams at 0x7ff4760aca50>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.maldi2resistance.data.driams import Driams\n",
    "\n",
    "driams = Driams(\n",
    "    root_dir=\"/home/jan/Uni/master/data/Driams\",\n",
    ")\n",
    "\n",
    "driams.loading_type = \"memory\"\n",
    "\n",
    "driams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451cb744-19f8-4d14-a270-b3827956e693",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1949dd07-64cd-4cd2-a9a1-66dfc4a3072b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.input = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer_1  = nn.Linear (hidden_dim, hidden_dim)\n",
    "        self.layer_2  = nn.Linear (hidden_dim, latent_dim)\n",
    "        #self.FC_mean  = nn.Linear(hidden_dim, latent_dim)\n",
    "        #self.FC_var   = nn.Linear (hidden_dim, latent_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "        \n",
    "        self.training = True\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_       = self.LeakyReLU(self.input(x))\n",
    "        h_       = self.LeakyReLU(self.layer_1(h_))\n",
    "        h_       = self.LeakyReLU(self.layer_2(h_))\n",
    "        \n",
    "        #mean     = self.FC_mean(h_)\n",
    "        #log_var  = self.FC_var(h_)  \n",
    "\n",
    "        return h_\n",
    "        return mean, log_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdbb04e-7604-4bcc-b292-f4bd2838a7ec",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34824602-971e-4902-a20e-f2753ce21ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.layer_1 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.layer_2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2)\n",
    "\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)      \n",
    "        z = mean + var*epsilon\n",
    "        return z\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h_    = self.LeakyReLU(self.input(x))\n",
    "        h_    = self.LeakyReLU(self.layer_1(h_))\n",
    "        \n",
    "        output = torch.sigmoid(self.layer_2(h_))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d020a8-0832-45ad-b96a-10986c325aad",
   "metadata": {},
   "source": [
    "### Combine Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2371f6c5-8021-450d-bca7-e57eddd2e916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, Encoder, Decoder):\n",
    "        super(Model, self).__init__()\n",
    "        self.Encoder = Encoder\n",
    "        self.Decoder = Decoder\n",
    "\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(DEVICE)        # sampling epsilon        \n",
    "        z = mean + var*epsilon                          # reparameterization trick\n",
    "        return z\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        latent = self.Encoder(x)\n",
    "        output = self.Decoder(latent)\n",
    "\n",
    "        return latent, output\n",
    "\n",
    "        \n",
    "        mean, log_var = self.Encoder(x)\n",
    "        z = self.reparameterization(mean, torch.exp(0.5 * log_var))\n",
    "        \n",
    "        x_hat   = self.Decoder(z)\n",
    "        \n",
    "        return x_hat, mean, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd28ab5fd2a857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(driams.label_stats.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81bd526-6a9c-4520-b121-9c01b4eb804a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (Encoder): Encoder(\n",
       "    (input): Linear(in_features=18000, out_features=4096, bias=True)\n",
       "    (layer_1): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (layer_2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "    (LeakyReLU): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (Decoder): Decoder(\n",
       "    (input): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "    (layer_1): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (layer_2): Linear(in_features=4096, out_features=34, bias=True)\n",
       "    (LeakyReLU): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(input_dim=18000, hidden_dim=4096, latent_dim=2048)\n",
    "decoder = Decoder(latent_dim=2048, hidden_dim = 4096, output_dim = len(driams.selected_antibiotics))\n",
    "\n",
    "model = Model(Encoder=encoder, Decoder=decoder)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361914c-9fb8-4a94-bb2c-95a3b68cda0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e04927-8b3b-4278-9203-1306b0588e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = torch.Generator()\n",
    "gen.manual_seed(SEED)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_size = int(0.8 * len(driams))\n",
    "test_size = len(driams) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(driams, [train_size, test_size], generator=gen)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96643a0-bd63-437b-a676-f3a4a1826910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463ac5c0db8646f693ec0f1414bc08b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAverage Loss: 0.001191 \tLearning rate: 0.001000\n",
      "\tAverage Loss: 0.001033 \tLearning rate: 0.001000\n",
      "\tAverage Loss: 0.000978 \tLearning rate: 0.001000\n",
      "\tAverage Loss: 0.000920 \tLearning rate: 0.001000\n",
      "\tAverage Loss: 0.000887 \tLearning rate: 0.001000\n",
      "\tAverage Loss: 0.000861 \tLearning rate: 0.001000\n",
      "\tAverage Loss: 0.000843 \tLearning rate: 0.001000\n",
      "\tAverage Loss: 0.000821 \tLearning rate: 0.001000\n",
      "\tAverage Loss: 0.000786 \tLearning rate: 0.001000\n",
      "\tAverage Loss: 0.000767 \tLearning rate: 0.000500\n",
      "\tAverage Loss: 0.000685 \tLearning rate: 0.000500\n",
      "\tAverage Loss: 0.000663 \tLearning rate: 0.000500\n",
      "\tAverage Loss: 0.000646 \tLearning rate: 0.000500\n",
      "\tAverage Loss: 0.000638 \tLearning rate: 0.000500\n",
      "\tAverage Loss: 0.000632 \tLearning rate: 0.000500\n",
      "\tAverage Loss: 0.000619 \tLearning rate: 0.000500\n",
      "\tAverage Loss: 0.000600 \tLearning rate: 0.000500\n",
      "\tAverage Loss: 0.000579 \tLearning rate: 0.000500\n",
      "\tAverage Loss: 0.000564 \tLearning rate: 0.000500\n",
      "\tAverage Loss: 0.000558 \tLearning rate: 0.000250\n",
      "\tAverage Loss: 0.000498 \tLearning rate: 0.000250\n",
      "\tAverage Loss: 0.000471 \tLearning rate: 0.000250\n",
      "\tAverage Loss: 0.000464 \tLearning rate: 0.000250\n",
      "\tAverage Loss: 0.000447 \tLearning rate: 0.000250\n",
      "\tAverage Loss: 0.000444 \tLearning rate: 0.000250\n",
      "\tAverage Loss: 0.000421 \tLearning rate: 0.000250\n",
      "\tAverage Loss: 0.000414 \tLearning rate: 0.000250\n",
      "\tAverage Loss: 0.000402 \tLearning rate: 0.000250\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "print(\"Start training ...\")\n",
    "model.train()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, amsgrad = True)\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "loss_per_batch = []\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "class_weights_negative = torch.tensor((1 - (driams.label_stats.loc[\"negative\"] / driams.label_stats.loc[\"n_sum\"])).values, device=DEVICE)\n",
    "class_weights_positive = torch.tensor((1 - (driams.label_stats.loc[\"positive\"] / driams.label_stats.loc[\"n_sum\"])).values, device=DEVICE)\n",
    "\n",
    "for epoch in tqdm(range(30)):\n",
    "    overall_loss = 0\n",
    "    \n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        \n",
    "        \n",
    "\n",
    "        x = x.view(batch_size, 18000)\n",
    "        x = x.to(DEVICE)\n",
    "        \n",
    "        split1,split2 = torch.chunk(x, 2)\n",
    "        combined =torch.add(split1 , split2)\n",
    "        combined_features = torch.div(combined, 2)\n",
    "        \n",
    "        x = torch.cat((x, combined_features), dim = 0)\n",
    "        \n",
    "        y = y.view(batch_size, len(driams.selected_antibiotics))\n",
    "        y = y.to(DEVICE)\n",
    "        \n",
    "        split1,split2 = torch.chunk(y, 2)\n",
    "        combined =torch.add(split1 , split2)\n",
    "        combined_labels = torch.div(combined, 2)\n",
    "        combined_labels[combined_labels == 0.5] =1\n",
    "        \n",
    "        y = torch.cat((y, combined_labels), dim = 0)\n",
    "        \n",
    "        positive_weight = torch.clone(y)\n",
    "        negative_weight = torch.clone(y)\n",
    "        negative_weight[negative_weight == 1] = -1\n",
    "        negative_weight[negative_weight == 0] = 1\n",
    "        negative_weight[negative_weight == -1] = 0\n",
    "        negative_weight = class_weights_negative * negative_weight[:, None]\n",
    "        positive_weight = class_weights_positive * positive_weight[:, None]\n",
    "        \n",
    "        weight = torch.add(positive_weight, negative_weight)\n",
    "        weight = torch.nan_to_num(weight, 0)\n",
    "        weight = weight[:,0, :]\n",
    "        \n",
    "        weight.to(DEVICE)\n",
    "        y = torch.nan_to_num(y, 0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # output, mean, log_var = model(x)\n",
    "        latent, output = model(x)\n",
    "\n",
    "        #loss = loss_function(y, output, mean, log_var)\n",
    "        loss = F.binary_cross_entropy(output, y, weight=weight)\n",
    "        loss = criterion(output, y)\n",
    "        current_loss_value = loss.item()\n",
    "        loss_per_batch.append(current_loss_value)\n",
    "        \n",
    "        overall_loss += current_loss_value\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    with tqdm.external_write_mode():\n",
    "        print(f\"\\tAverage Loss: {overall_loss / (batch_idx*batch_size):.6f} \\tLearning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "\n",
    "print(\"Finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f78615-fd71-43f2-a7a1-c12f17be8950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (Encoder): Encoder(\n",
       "    (input): Linear(in_features=18000, out_features=4096, bias=True)\n",
       "    (layer_1): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (layer_2): Linear(in_features=4096, out_features=2048, bias=True)\n",
       "    (LeakyReLU): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       "  (Decoder): Decoder(\n",
       "    (input): Linear(in_features=2048, out_features=4096, bias=True)\n",
       "    (layer_1): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (layer_2): Linear(in_features=4096, out_features=34, bias=True)\n",
       "    (LeakyReLU): LeakyReLU(negative_slope=0.2)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645c4916b1549621",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(model)\n",
    "model_scripted.save('./model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
