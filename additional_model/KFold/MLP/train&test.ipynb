{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d89aed94233e3ff",
   "metadata": {},
   "source": [
    "# Fully connected feedforward network implementing a loss mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaaf570-0600-4d60-9fde-fcebcff3ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics.classification\n",
    "from torch import cuda\n",
    "assert cuda.is_available()\n",
    "assert cuda.device_count() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e50fb9-601f-425a-bf79-6441170b27dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "print(cuda.get_device_name(cuda.current_device()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4b645f-6b5e-428e-855c-64b7e33ff874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a6650-ef2b-44f1-9ea8-78912b15e9a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f94095aafb0>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\")\n",
    "SEED = 76436278\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b5a248-98cc-42cc-99b2-49bdd5ca1f35",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00707ac6b2975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aad1516440d849d1a93655b817a818d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading Spectra into Memory:   0%|          | 0/55780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <table>\n",
       "            <thead>\n",
       "                <tr>\n",
       "                    <th>Antibiotic:</th>\n",
       "                    <th> Amikacin </th><th> Amoxicillin-Clavulanic acid </th><th> Ampicillin </th><th> Ampicillin-Amoxicillin </th><th> Aztreonam </th><th> Benzylpenicillin </th><th> Cefazolin </th><th> Cefepime </th><th> Cefpodoxime </th><th> Ceftazidime </th><th> Ceftriaxone </th><th> Cefuroxime </th><th> Ciprofloxacin </th><th> Clarithromycin </th><th> Clindamycin </th><th> Colistin </th><th> Cotrimoxazole </th><th> Ertapenem </th><th> Erythromycin </th><th> Fosfomycin </th><th> Fosfomycin-Trometamol </th><th> Fusidic acid </th><th> Gentamicin </th><th> Imipenem </th><th> Levofloxacin </th><th> Meropenem </th><th> Mupirocin </th><th> Nitrofurantoin </th><th> Norfloxacin </th><th> Oxacillin </th><th> Penicillin </th><th> Piperacillin-Tazobactam </th><th> Polymyxin B </th><th> Rifampicin </th><th> Teicoplanin </th><th> Tetracycline </th><th> Tobramycin </th><th> Vancomycin </th>\n",
       "                </tr>\n",
       "            </thead>\n",
       "            <tbody>\n",
       "                <tr>\n",
       "                    <td>Number resistant:</td>\n",
       "                    <td> 1068 </td><td> 13366 </td><td> 8578 </td><td> 21966 </td><td> 628 </td><td> 618 </td><td> 4223 </td><td> 7383 </td><td> 2338 </td><td> 3470 </td><td> 8659 </td><td> 5855 </td><td> 9338 </td><td> 310 </td><td> 4381 </td><td> 2874 </td><td> 7405 </td><td> 427 </td><td> 5468 </td><td> 2303 </td><td> 1326 </td><td> 3620 </td><td> 3481 </td><td> 7560 </td><td> 4217 </td><td> 5194 </td><td> 570 </td><td> 1271 </td><td> 1205 </td><td> 5537 </td><td> 12431 </td><td> 7616 </td><td> 486 </td><td> 580 </td><td> 244 </td><td> 3534 </td><td> 1707 </td><td> 227 </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td>Number susceptible:</td>\n",
       "                    <td> 20941 </td><td> 24992 </td><td> 4194 </td><td> 4905 </td><td> 456 </td><td> 457 </td><td> 5813 </td><td> 31567 </td><td> 4382 </td><td> 24566 </td><td> 28464 </td><td> 8368 </td><td> 36822 </td><td> 1262 </td><td> 9841 </td><td> 15784 </td><td> 24590 </td><td> 21740 </td><td> 9044 </td><td> 10184 </td><td> 4803 </td><td> 8498 </td><td> 22662 </td><td> 31717 </td><td> 17989 </td><td> 27228 </td><td> 4656 </td><td> 3603 </td><td> 7031 </td><td> 7740 </td><td> 4286 </td><td> 31308 </td><td> 2305 </td><td> 14964 </td><td> 8486 </td><td> 10376 </td><td> 16809 </td><td> 20540 </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td>Number data points:</td>\n",
       "                    <td> 22009 </td><td> 38358 </td><td> 12772 </td><td> 26871 </td><td> 1084 </td><td> 1075 </td><td> 10036 </td><td> 38950 </td><td> 6720 </td><td> 28036 </td><td> 37123 </td><td> 14223 </td><td> 46160 </td><td> 1572 </td><td> 14222 </td><td> 18658 </td><td> 31995 </td><td> 22167 </td><td> 14512 </td><td> 12487 </td><td> 6129 </td><td> 12118 </td><td> 26143 </td><td> 39277 </td><td> 22206 </td><td> 32422 </td><td> 5226 </td><td> 4874 </td><td> 8236 </td><td> 13277 </td><td> 16717 </td><td> 38924 </td><td> 2791 </td><td> 15544 </td><td> 8730 </td><td> 13910 </td><td> 18516 </td><td> 20767 </td>\n",
       "                </tr>\n",
       "            </tbody>\n",
       "        </table>\n",
       "                "
      ],
      "text/plain": [
       "<src.maldi2resistance.data.driams.Driams at 0x7f92ed2ecd90>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.maldi2resistance.data.driams import Driams\n",
    "\n",
    "driams = Driams(\n",
    "    root_dir=\"/home/jan/Uni/master/data/Driams\",\n",
    ")\n",
    "\n",
    "driams.loading_type = \"memory\"\n",
    "\n",
    "driams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbd28ab5fd2a857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(driams.label_stats.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81bd526-6a9c-4520-b121-9c01b4eb804a",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 7.69 GiB of which 265.19 MiB is free. Process 12195 has 468.00 MiB memory in use. Process 16127 has 892.00 MiB memory in use. Process 55778 has 348.00 MiB memory in use. Process 76982 has 348.00 MiB memory in use. Process 198113 has 346.00 MiB memory in use. Process 204107 has 502.00 MiB memory in use. Process 204143 has 866.00 MiB memory in use. Including non-PyTorch memory, this process has 1016.00 MiB memory in use. Of the allocated memory 884.72 MiB is allocated by PyTorch, and 1.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m AeBasedMLP(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m18000\u001b[39m, output_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(driams\u001b[38;5;241m.\u001b[39mselected_antibiotics), hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m, latent_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m)\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m----> 7\u001b[0m model_state \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(model\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "File \u001b[0;32m~/anaconda3/envs/master_backend/lib/python3.11/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[38;5;241m=\u001b[39m _reconstruct(x, memo, \u001b[38;5;241m*\u001b[39mrv)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/anaconda3/envs/master_backend/lib/python3.11/copy.py:297\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m dictiter:\n\u001b[1;32m    296\u001b[0m         key \u001b[38;5;241m=\u001b[39m deepcopy(key, memo)\n\u001b[0;32m--> 297\u001b[0m         value \u001b[38;5;241m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    298\u001b[0m         y[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/master_backend/lib/python3.11/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m copier(memo)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/master_backend/lib/python3.11/site-packages/torch/_tensor.py:122\u001b[0m, in \u001b[0;36mTensor.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    114\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe default implementation of __deepcopy__() for wrapper subclasses \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly works for subclass types that implement clone() and for which \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferent type.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         )\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     new_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_deepcopy(memo)\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_quantized:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;66;03m# quantizer_params can be different type based on torch attribute\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         quantizer_params: Union[\n\u001b[1;32m    126\u001b[0m             Tuple[torch\u001b[38;5;241m.\u001b[39mqscheme, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    127\u001b[0m             Tuple[torch\u001b[38;5;241m.\u001b[39mqscheme, Tensor, Tensor, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m    128\u001b[0m         ]\n",
      "File \u001b[0;32m~/anaconda3/envs/master_backend/lib/python3.11/site-packages/torch/storage.py:907\u001b[0m, in \u001b[0;36mTypedStorage._deepcopy\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_deepcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, memo):\n\u001b[0;32m--> 907\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_wrapped_storage(copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_untyped_storage, memo))\n",
      "File \u001b[0;32m~/anaconda3/envs/master_backend/lib/python3.11/copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m copier(memo)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/master_backend/lib/python3.11/site-packages/torch/storage.py:143\u001b[0m, in \u001b[0;36m_StorageBase.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cdata \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m memo[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cdata]\n\u001b[0;32m--> 143\u001b[0m new_storage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone()\n\u001b[1;32m    144\u001b[0m memo[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cdata] \u001b[38;5;241m=\u001b[39m new_storage\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/master_backend/lib/python3.11/site-packages/torch/storage.py:157\u001b[0m, in \u001b[0;36m_StorageBase.clone\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclone\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a copy of this storage.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnbytes(), device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mcopy_(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 7.69 GiB of which 265.19 MiB is free. Process 12195 has 468.00 MiB memory in use. Process 16127 has 892.00 MiB memory in use. Process 55778 has 348.00 MiB memory in use. Process 76982 has 348.00 MiB memory in use. Process 198113 has 346.00 MiB memory in use. Process 204107 has 502.00 MiB memory in use. Process 204143 has 866.00 MiB memory in use. Including non-PyTorch memory, this process has 1016.00 MiB memory in use. Of the allocated memory 884.72 MiB is allocated by PyTorch, and 1.28 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from maldi2resistance.model.MLP import AeBasedMLP\n",
    "import copy\n",
    "\n",
    "model = AeBasedMLP(input_dim=18000, output_dim=len(driams.selected_antibiotics), hidden_dim=4096, latent_dim=2048)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "model_state = copy.deepcopy(model.state_dict()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6f3eacba0f0d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e361914c-9fb8-4a94-bb2c-95a3b68cda0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96643a0-bd63-437b-a676-f3a4a1826910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from maldi2resistance.metric.PrecisionRecall import MultiLabelPRNan\n",
    "from pathlib import Path\n",
    "from maldi2resistance.metric.ROC import MultiLabelRocNan\n",
    "from maldi2resistance.loss.maskedLoss import MaskedBCE\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "print(\"Start training ...\")\n",
    "model.train()\n",
    "\n",
    "batch_size = 128\n",
    "fig_path = Path(\"./kfold/figures\")\n",
    "fig_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "loss_per_batch = []\n",
    "\n",
    "class_weights_negative = torch.tensor((1 - (driams.label_stats.loc[\"negative\"] / driams.label_stats.loc[\"n_sum\"])).values, device=DEVICE)\n",
    "class_weights_positive = torch.tensor((1 - (driams.label_stats.loc[\"positive\"] / driams.label_stats.loc[\"n_sum\"])).values, device=DEVICE)\n",
    "\n",
    "criterion = MaskedBCE(class_weights_positive= class_weights_negative, class_weights_negative= class_weights_negative)\n",
    "\n",
    "gen = torch.Generator()\n",
    "\n",
    "for fold, (train_data, test_data) in enumerate(driams.getK_fold(n_splits=5, shuffle=True, random_state= SEED)):\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True, generator= gen.manual_seed(SEED))\n",
    "    test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=True, drop_last=True, generator= gen.manual_seed(SEED))\n",
    "    \n",
    "    model.load_state_dict(model_state)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr=1e-3, amsgrad = True)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    for epoch in tqdm(range(30)):\n",
    "        overall_loss = 0\n",
    "        \n",
    "        for batch_idx, (x, y) in enumerate(train_loader):\n",
    "    \n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            output = model(x)\n",
    "        \n",
    "            loss = criterion(output, y)\n",
    "            current_loss_value = loss.item()\n",
    "            loss_per_batch.append(current_loss_value)\n",
    "            \n",
    "            overall_loss += current_loss_value\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        scheduler.step()\n",
    "        with tqdm.external_write_mode():\n",
    "            print(f\"\\tAverage Loss: {overall_loss / (batch_idx*batch_size):.6f} \\tLearning rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "    \n",
    "    \n",
    "    print(f\"Finished Fold {fold}\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    test_features, test_labels = next(iter(test_loader))\n",
    "    test_features = test_features.to(DEVICE)\n",
    "    test_labels = test_labels.to(DEVICE)\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    ml_roc = MultiLabelRocNan()\n",
    "    output = model(test_features)\n",
    "    \n",
    "    \n",
    "    ml_roc.compute(output,test_labels,driams.selected_antibiotics, create_csv=f\"./kfold/csv/fold-{fold}_ROC.csv\")\n",
    "    fig_, ax_ = ml_roc()\n",
    "    \n",
    "    plt.savefig(fig_path / f\"fold-{fold}_ROC.png\", transparent=True, format= \"png\", bbox_inches = \"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    ml_pr = MultiLabelPRNan()\n",
    "    ml_pr.compute(output,test_labels,driams.selected_antibiotics, create_csv=f\"./kfold/csv/fold-{fold}_PrecisionRecall.csv\")\n",
    "    \n",
    "    fig_, ax_ = ml_pr()\n",
    "    \n",
    "    plt.savefig(fig_path / f\"fold-{fold}_PrecisionRecall.png\", transparent=True, format= \"png\", bbox_inches = \"tight\")\n",
    "    plt.close()\n",
    "    \n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f600e6963c560e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "micro = []\n",
    "macro = []\n",
    "\n",
    "for fold in range(0,5):\n",
    "    csv = pandas.read_csv(f\"./kfold/csv/fold-{fold}_ROC.csv\")\n",
    "    micro.append(csv[csv[\"class\"] == \"micro\"][\"ROCAUC\"])\n",
    "    macro.append(csv[csv[\"class\"] == \"macro\"][\"ROCAUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb9df2adbe214c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean\t: 0.923254644870758\n",
      " SD \t: 0.0031201617094989494\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"Mean\\t: {np.mean(micro)}\")\n",
    "print(f\" SD \\t: {np.std(micro) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef0009f0d5188f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean\t: 0.8998323920526003\n",
      " SD \t: 0.0019176116481031224\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean\\t: {np.mean(macro)}\")\n",
    "print(f\" SD \\t: {np.std(macro) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ffecb8ed7b801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "micro = []\n",
    "macro = []\n",
    "\n",
    "for fold in range(0,5):\n",
    "    csv = pandas.read_csv(f\"./kfold/csv/fold-{fold}_PrecisionRecall.csv\")\n",
    "    micro.append(csv[csv[\"class\"] == \"micro\"][\"PrecisionRecallAUC\"])\n",
    "    macro.append(csv[csv[\"class\"] == \"macro\"][\"PrecisionRecallAUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ef9d9c2528e40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean\t: 0.8620764374732971\n",
      " SD \t: 0.004877474018859148\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"Mean\\t: {np.mean(micro)}\")\n",
    "print(f\" SD \\t: {np.std(micro) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beaf511fa04a85e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean\t: 0.7627670513956171\n",
      " SD \t: 0.005410371951255511\n"
     ]
    }
   ],
   "source": [
    "print(f\"Mean\\t: {np.mean(macro)}\")\n",
    "print(f\" SD \\t: {np.std(macro) }\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
